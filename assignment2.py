# -*- coding: utf-8 -*-
"""assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cggfnpwkrEC2qRzvaJWgnGr71bKw8aep
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load training data
train_url = "https://github.com/dustywhite7/Econ8310/raw/master/AssignmentData/assignment3.csv"
df = pd.read_csv(train_url)

# Load test data
test_url = "https://github.com/dustywhite7/Econ8310/raw/master/AssignmentData/assignment3test.csv"
test_df = pd.read_csv(test_url)

# Drop non-essential columns if they exist
if 'id' in df.columns and 'DateTime' in df.columns:
    df = df.drop(columns=['id', 'DateTime'])
if 'id' in test_df.columns and 'DateTime' in test_df.columns:
    test_df = test_df.drop(columns=['id', 'DateTime'])

# Identify categorical variables and apply one-hot encoding
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)

# Align test data with training data
X = df.drop(columns=["meal"])  # Features
y = df["meal"].astype(int)  # Ensure target is integer
X_test = test_df.drop(columns=["meal"], errors='ignore')

# Ensure test set has the same columns as the training set
X_test = X_test.reindex(columns=X.columns, fill_value=0)

# Split training data for model evaluation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    "DecisionTree": DecisionTreeClassifier(random_state=42),
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),
    "BoostedTree": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

# Train and evaluate models
best_model = None
best_accuracy = 0

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    acc = accuracy_score(y_val, y_pred)
    print(f"{name} Accuracy: {acc:.4f}")

    if acc > best_accuracy:
        best_accuracy = acc
        best_model = model

# Ensure 'model' is a valid classifier
assert isinstance(best_model, (DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier)), \
    "Error: model is not a valid classifier!"

# Save the best model
joblib.dump(best_model, "model.pkl")
model = best_model  # Required for test case validation
modelFit = best_model  # Required variable name

# Make predictions on test data
pred = best_model.predict(X_test)

# Ensure predictions are valid (binary integers)
pred = np.clip(pred, 0, 1)  # Force predictions to be 0 or 1
pred = np.array(pred, dtype=int)  # Convert to integer NumPy array

# Ensure exactly 1000 predictions
assert len(pred) == 1000, "Error: Predictions must contain exactly 1000 values!"
assert set(np.unique(pred)).issubset({0, 1}), "Error: Predictions should only contain 0s and 1s!"

# Save predictions
predictions_df = pd.DataFrame(pred, columns=["meal_prediction"])
predictions_df.to_csv("predictions.csv", index=False)

print("âœ… Model training complete. Predictions saved as 'predictions.csv'.")