# -*- coding: utf-8 -*-
"""assignment2main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1074QxLiFnOEoSEGHz9_BnYZ-wWIqXAHU
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load training data
train_url = "https://github.com/dustywhite7/Econ8310/raw/master/AssignmentData/assignment3.csv"
df = pd.read_csv(train_url)

# Load test data
test_url = "https://github.com/dustywhite7/Econ8310/raw/master/AssignmentData/assignment3test.csv"
test_df = pd.read_csv(test_url)

# Drop non-essential columns if they exist
if 'id' in df.columns and 'DateTime' in df.columns:
    df = df.drop(columns=['id', 'DateTime'])
if 'id' in test_df.columns and 'DateTime' in test_df.columns:
    test_df = test_df.drop(columns=['id', 'DateTime'])

# Identify categorical variables and apply one-hot encoding
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)

# Align test data with training data
X = df.drop(columns=["meal"])  # Features
y = df["meal"].astype(int)  # Ensure target is integer
X_test = test_df.drop(columns=["meal"], errors='ignore')
X_test = X_test.reindex(columns=X.columns, fill_value=0)

# Split training data for model evaluation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    "DecisionTree": DecisionTreeClassifier(random_state=42),
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),
    "BoostedTree": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

# Train and evaluate models
best_model = DecisionTreeClassifier(random_state=42)  # Default model to avoid NoneType errors
best_accuracy = 0

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    acc = accuracy_score(y_val, y_pred)
    print(f"{name} Accuracy: {acc:.4f}")

    if acc > best_accuracy:
        best_accuracy = acc
        best_model = model

# Debugging: Print selected best model type
print(f"Selected best model type: {type(best_model)}")

# Ensure 'model' is a valid classifier
assert isinstance(best_model, (DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier)), \
    f"Error: model is not a valid classifier! Found {type(best_model)}"

# Save the best model
joblib.dump(best_model, "model.pkl")
model = best_model  # Required for test case validation
modelFit = best_model  # Required variable name

# Make predictions on test data
pred = best_model.predict(X_test)

# Ensure predictions are valid (binary integers)
pred = np.clip(pred, 0, 1)  # Force predictions to be 0 or 1
pred = np.array(pred, dtype=int).tolist()  # Convert to integer list for test compatibility

# Ensure exactly 1000 predictions
assert len(pred) == 1000, "Error: Predictions must contain exactly 1000 values!"
assert all(isinstance(i, int) for i in pred), "Error: Predictions must be strictly integers!"
assert set(pred).issubset({0, 1}), "Error: Predictions should only contain 0s and 1s!"

print("✅ Model training complete. Predictions are stored in the 'pred' variable.")

# Additional Debugging: Check if model is properly fitted
if hasattr(modelFit, 'tree_'):  # Decision Tree & Random Forest
    print("✅ Model is fitted (tree structure exists).")
elif hasattr(modelFit, 'feature_importances_'):  # Works for Gradient Boosting
    print("✅ Model is fitted (feature importance exists).")
elif hasattr(modelFit, '_Booster'):  # XGBoost (if used in future)
    print("✅ Model is fitted (XGBoost Booster exists).")
else:
    raise AssertionError("❌ Error: Model does not appear to be fitted correctly!")